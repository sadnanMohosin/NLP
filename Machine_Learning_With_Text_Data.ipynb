{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Natural Language Processing (or NLP) is ubiquitous and has multiple applications. A few examples include email classification into spam and ham, chatbots, AI agents, social media analysis, and classifying customer or employee feedback into Positive, Negative or Neutral.\n",
    "\n",
    "In this guide, we will take up an extremely popular use case of NLP - building a supervised machine learning model on text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "In this guide, we will take up the task of automating reviews in medicine. The medical literature is voluminous and rapidly changing, increasing the need for reviews. Often, such reviews are done manually, which is tedious and time-consuming. We will try to address this problem by building a text classification model which will automate the process.\n",
    "\n",
    "The dataset nlpdata2.csv we will use comes from a Pubmed search, and contains 1748 observations and 3 variables, as described below:\n",
    "\n",
    "    title - consists of the titles of papers retrieved\n",
    "\n",
    "    abstract - consists of the abstracts of papers retrieved\n",
    "\n",
    "    trial - variable indicating whether the paper is a clinical trial testing a drug therapy for cancer.\n",
    "    class - like the variable 'trial', indicating whether the paper is a clinical trial (Yes) or not (No). This is the target variable and was added in the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "Following are the steps we will follow in this guide.\n",
    "\n",
    "Step 1 - Loading the required libraries and modules.\n",
    "\n",
    "Step 2 - Loading the data and performing basic data checks.\n",
    "\n",
    "Step 3 - Pre-processing the raw text and getting it ready for machine learning.\n",
    "\n",
    "Step 4 - Creating the Training and Test datasets.\n",
    "\n",
    "Step 5 - Converting text to word frequency vectors with TfidfVectorizer.\n",
    "\n",
    "Step 6 - Create and fit the classifier.\n",
    "\n",
    "Step 7 - Computing the evaluation metrics.\n",
    "\n",
    "The following sections will cover these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Loading the Required Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Loading the Data and Performing Basic Data Checks\n",
    "\n",
    "The first line of code reads in the data as pandas data frame, while the second line prints the shape - 1,748 observations of 4 variables. The third line prints the first five observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1748, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>trial</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tetracosactrin vs. methylprednisolone in the p...</td>\n",
       "      <td>0.5 mg tetracosactrin is considered to be equi...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cyclic combination chemotherapy for metastatic...</td>\n",
       "      <td>104 nonrandomized patients suffering from meta...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reduced morbidity from skeletal metastases in ...</td>\n",
       "      <td>131 patients with osteolytic metastases from b...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comparison of short-term and continuous chemot...</td>\n",
       "      <td>132 patients with advanced recurrent breast ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A randomised phase III cross-over study of tam...</td>\n",
       "      <td>139 peri- and postmenopausal women with advanc...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Tetracosactrin vs. methylprednisolone in the p...   \n",
       "1  Cyclic combination chemotherapy for metastatic...   \n",
       "2  Reduced morbidity from skeletal metastases in ...   \n",
       "3  Comparison of short-term and continuous chemot...   \n",
       "4  A randomised phase III cross-over study of tam...   \n",
       "\n",
       "                                            abstract  trial class  \n",
       "0  0.5 mg tetracosactrin is considered to be equi...      0    No  \n",
       "1  104 nonrandomized patients suffering from meta...      1   Yes  \n",
       "2  131 patients with osteolytic metastases from b...      0    No  \n",
       "3  132 patients with advanced recurrent breast ca...      1   Yes  \n",
       "4  139 peri- and postmenopausal women with advanc...      1   Yes  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'F:\\work in jupyter\\NLP\\datasets\\nlpdata2.csv')\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, we will do basic data exploration. Let us check the distribution of the target class which can be done using barplot. The first line of code below groups the 'class' variables by counting the number of their occurrences. The second line displays the barplot.\n",
    "\n",
    "It is evident that we have more occurrences of 'No' than 'Yes' in the target variable. Still, the good thing is that the difference is not significant and the data is relatively balanced.\n",
    "\n",
    "The baseline accuracy is calculated in the third line of code, which comes out to be 56%. It is calculated as the number of times the majority class (i.e., 'No') appears in the target variable, divided by the total number of observations.\n",
    "\n",
    "The baseline accuracy is important but often ignored in machine learning. It sets the benchmark in terms of minimum accuracy which the model should achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5554919908466819\n"
     ]
    }
   ],
   "source": [
    "df.groupby('class').abstract.count().plot.bar(ylim=0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(971/1748) #Baseline accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 – Pre-processing the Raw Text and Getting It Ready for Machine Learning\n",
    "\n",
    "Now, we are ready to build our text classifier. However, this is where things begin to get trickier in NLP. The data we have is in raw text which by itself, cannot be used as features. So, we will have to pre-process the text.\n",
    "\n",
    "The common pre-processing steps are:\n",
    "\n",
    "Removing punctuation - the rule of thumb is to remove everything that is not in the form x,y,z.\n",
    "\n",
    "Removing stopwords - these are unhelpful words like 'the', 'is', 'at'. These are not helpful because the frequency of such stopwords is high in the corpus, but they don't help in differentiating the target classes. The removal of Stopwords also reduces the data size.\n",
    "\n",
    "Conversion to lower case - words like 'Clinical' and 'clinical' need to be considered as one word. Hence, these are converted to lowercase.\n",
    "\n",
    "Stemming - the goal of stemming is to reduce the number of inflectional forms of words appearing in the text. This causes words such as “argue”, \"argued\", \"arguing\", \"argues\" to be reduced to their common stem “argu”. This helps in decreasing the size of the vocabulary space. There are many ways to perform Stemming, the popular one being the “Porter Stemmer” method by Martin Porter.\n",
    "\n",
    "For completing the above-mentioned steps, we will have to load the nltk package, which is done in the first line of code below. The second line downloads the list of 'stopwords' in the nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sadnan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With nltk package loaded and ready to use, we will perform the pre-processing tasks. The first two lines of code below imports the stopwords and the PorterStemmer modules, respectively.\n",
    "\n",
    "The third line imports the regular expressions library, ‘re’, which is a powerful python package for text parsing. \n",
    "\n",
    "The fourth to sixth lines of code does the text pre-processing discussed above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "df['processedtext'] = df['abstract'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at the pre-processed data set that has a new column 'processedtext'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1748, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>trial</th>\n",
       "      <th>class</th>\n",
       "      <th>processedtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tetracosactrin vs. methylprednisolone in the p...</td>\n",
       "      <td>0.5 mg tetracosactrin is considered to be equi...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>mg tetracosactrin consid equival mg methylpred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cyclic combination chemotherapy for metastatic...</td>\n",
       "      <td>104 nonrandomized patients suffering from meta...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>nonrandom patient suffer metastat breast cance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reduced morbidity from skeletal metastases in ...</td>\n",
       "      <td>131 patients with osteolytic metastases from b...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>patient osteolyt metastas breast cancer random...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comparison of short-term and continuous chemot...</td>\n",
       "      <td>132 patients with advanced recurrent breast ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>patient advanc recurr breast cancer treat four...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A randomised phase III cross-over study of tam...</td>\n",
       "      <td>139 peri- and postmenopausal women with advanc...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>peri postmenopaus women advanc recurr breast c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17ss-Hydroxysteroid dehydrogenase type 1 as pr...</td>\n",
       "      <td>17ss-Hydroxysteroid dehydrogenases (17HSDs) ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>ss hydroxysteroid dehydrogenas hsd involv loca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vindesine-mitoxantrone (VM) versus vindesine-4...</td>\n",
       "      <td>182 patients with metastatic breast cancer wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>patient metastat breast cancer random v mg v m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Phase I study of L-asparaginase (NSC 109229).</td>\n",
       "      <td>22 patients received intravenously infused L-a...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>patient receiv intraven infus l asparaginas es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Adjuvant chemotherapy in the treatment of brea...</td>\n",
       "      <td>248 patients with locally radically treated ea...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>patient local radic treat earli breast cancer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Scalp cooling has no place in the prevention o...</td>\n",
       "      <td>35 patients were studied to determine the effe...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>patient studi determin effect scalp hypothermi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Tetracosactrin vs. methylprednisolone in the p...   \n",
       "1  Cyclic combination chemotherapy for metastatic...   \n",
       "2  Reduced morbidity from skeletal metastases in ...   \n",
       "3  Comparison of short-term and continuous chemot...   \n",
       "4  A randomised phase III cross-over study of tam...   \n",
       "5  17ss-Hydroxysteroid dehydrogenase type 1 as pr...   \n",
       "6  Vindesine-mitoxantrone (VM) versus vindesine-4...   \n",
       "7      Phase I study of L-asparaginase (NSC 109229).   \n",
       "8  Adjuvant chemotherapy in the treatment of brea...   \n",
       "9  Scalp cooling has no place in the prevention o...   \n",
       "\n",
       "                                            abstract  trial class  \\\n",
       "0  0.5 mg tetracosactrin is considered to be equi...      0    No   \n",
       "1  104 nonrandomized patients suffering from meta...      1   Yes   \n",
       "2  131 patients with osteolytic metastases from b...      0    No   \n",
       "3  132 patients with advanced recurrent breast ca...      1   Yes   \n",
       "4  139 peri- and postmenopausal women with advanc...      1   Yes   \n",
       "5  17ss-Hydroxysteroid dehydrogenases (17HSDs) ar...      0    No   \n",
       "6  182 patients with metastatic breast cancer wer...      1   Yes   \n",
       "7  22 patients received intravenously infused L-a...      0    No   \n",
       "8  248 patients with locally radically treated ea...      1   Yes   \n",
       "9  35 patients were studied to determine the effe...      0    No   \n",
       "\n",
       "                                       processedtext  \n",
       "0  mg tetracosactrin consid equival mg methylpred...  \n",
       "1  nonrandom patient suffer metastat breast cance...  \n",
       "2  patient osteolyt metastas breast cancer random...  \n",
       "3  patient advanc recurr breast cancer treat four...  \n",
       "4  peri postmenopaus women advanc recurr breast c...  \n",
       "5  ss hydroxysteroid dehydrogenas hsd involv loca...  \n",
       "6  patient metastat breast cancer random v mg v m...  \n",
       "7  patient receiv intraven infus l asparaginas es...  \n",
       "8  patient local radic treat earli breast cancer ...  \n",
       "9  patient studi determin effect scalp hypothermi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Creating the Training and Test Datasets\n",
    "\n",
    "The first line of code below imports the module for creating training and test data sets. The second line creates an array of the target variable, called 'target'.\n",
    "\n",
    "The third line creates the training (X_train, y_train) and test set (X-test, y_test) arrays. It keeps 30% of the data for testing the model. The 'random_state' argument ensures that the results are reproducible.\n",
    "\n",
    "The fourth line prints the shape of the overall, training and test dataset, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1748, 5)\n",
      "(1223,)\n",
      "(525,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "target = df['class']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['processedtext'], target, test_size=0.30, random_state=100)\n",
    "\n",
    "\n",
    "print(df.shape); print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Converting Text to Word Frequency Vectors with TfidfVectorizer.\n",
    "\n",
    "We have processed the text, but we need to convert it to word frequency vectors for building machine learning models. There are several ways to do this, such as using CountVectorizer and HashingVectorizer, but the TfidfVectorizer is the most popular one.\n",
    "\n",
    "TF-IDF is an acronym that stands for 'Term Frequency-Inverse Document Frequency'. It is used as a weighting factor in text mining applications.\n",
    "\n",
    "Term Frequency (TF): This summarizes the normalized Term Frequency within a document.\n",
    "\n",
    "Inverse Document Frequency (IDF): This reduces the weight of terms that appear a lot across documents. In simple terms, TF-IDF attempts to highlight important words which are frequent in a document but not across documents. We will work on creating TF-IDF vectors for our documents.\n",
    "\n",
    "The first line of code below imports the TfidfVectorizer from 'sklearn.feature_extraction.text' module. The second line initializes the TfidfVectorizer object, called 'vectorizer_tfidf'.\n",
    "\n",
    "The third line fits and transforms the training data. The fourth line of code transforms the test data, while the fifth line prints the first 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aacr', 'aag', 'aastrom', 'ab', 'abandon', 'abc', 'abcb', 'abcsg', 'abdomen']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "\n",
    "train_tfIdf = vectorizer_tfidf.fit_transform(X_train.values.astype('U'))\n",
    "\n",
    "\n",
    "test_tfIdf = vectorizer_tfidf.transform(X_test.values.astype('U'))\n",
    "\n",
    "\n",
    "print(vectorizer_tfidf.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shape of the transformed TF-IDF train and test datasets. The following line of code performs this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1223, 5602)\n",
      "(525, 5602)\n"
     ]
    }
   ],
   "source": [
    "print(train_tfIdf.shape); print(test_tfIdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Create and Fit the Classifier.\n",
    "\n",
    "Now, we will build the text classification model. The algorithm we will choose is the Naive Bayes Classifier, which is commonly used for text classification problems, as it is based on probability. It is simple and effective in answering questions such as \"Given a particular term in the document, what is the likely chance (probability) that it belongs to the particular class?\"\n",
    "\n",
    "We start by importing the necessary modules that is done in the first two lines of code below. The third line creates a Multinomial Naive Bayes classifier, called 'nb_classifier'. The fourth line of code fits the classifier on the training data.\n",
    "\n",
    "Finally, our model is trained and it is ready to generate predictions on the unseen data. This is performed in the fifth line of code, while the sixth line prints the predicted class for the first 10 records in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "\n",
    "nb_classifier.fit(train_tfIdf, y_train)\n",
    "\n",
    "\n",
    "pred2 = nb_classifier.predict(test_tfIdf) \n",
    "\n",
    "print(pred2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - Computing the Evaluation Metrics\n",
    "\n",
    "We are now ready to evaluate the performance of our model on test data. Using the 'metrics.accuracy_score’ function, we compute the accuracy in the first line of code below and print the result using the second line of code. We see that the accuracy is 86.5%, which is a good score.\n",
    "\n",
    "We can also calculate the accuracy through confusion metrics. The third line of code below creates the confusion metrics, where the 'labels' argument is used to specify the target class labels ('Yes' or 'No' in our case). The fourth line prints the confusion metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8647619047619047\n",
      "[[211  28]\n",
      " [ 43 243]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score: score\n",
    "\n",
    "accuracy_tfidf = metrics.accuracy_score(y_test, pred2)\n",
    "\n",
    "print(accuracy_tfidf)\n",
    "\n",
    "\n",
    "Conf_metrics_tfidf = metrics.confusion_matrix(y_test, pred2, labels=['Yes', 'No'])\n",
    "\n",
    "print(Conf_metrics_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the main diagonal results on the confusion matrix as the true labels, we can calculate the accuracy, which is 86.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8647619047619047"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(211+243)/(239+286)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Naïve Bayes Model\n",
    "\n",
    "At the beginning of the guide, we established the baseline accuracy of 55.5%. Our Naive Bayes model is conveniently beating this baseline model by achieving the accuracy score of 86.5%. This also sets a new benchmark for any new algorithm or model refinements. We will try out the Random Forest Algorithm to see if it improves our result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Random Forest Classifier\n",
    "\n",
    "The first two lines of code below import the necessary modules. The third line creates a Random Forest Classifier, while the fourth line fits the classifier on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 100)\n",
    "\n",
    "\n",
    "classifier.fit(train_tfIdf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model training is done, we use the model to generate predictions on the test data, which is done in the first line of code below. The second line prints the predicted class for the first 10 records in the test data. The third and fourth lines of code calculates and prints the accuracy score, respectively. We see that the accuracy dropped to 78.6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'No' 'Yes']\n",
      "0.7866666666666666\n",
      "[[187  52]\n",
      " [ 60 226]]\n"
     ]
    }
   ],
   "source": [
    "predRF = classifier.predict(test_tfIdf) \n",
    "\n",
    "print(predRF[:10])\n",
    "\n",
    "\n",
    "# Calculate the accuracy score\n",
    "\n",
    "accuracy_RF = metrics.accuracy_score(y_test, predRF)\n",
    "\n",
    "print(accuracy_RF)\n",
    "\n",
    "\n",
    "Conf_metrics_RF = metrics.confusion_matrix(y_test, predRF, labels=['Yes', 'No'])\n",
    "\n",
    "print(Conf_metrics_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this guide, you have learned the fundamentals of text cleaning and pre-processing, and building and evaluating text classification models using Naive Bayes and Random Forest Algorithms. The performance of the models is summarized below:\n",
    "\n",
    "    Baseline Model Accuracy - 56%\n",
    "\n",
    "    Accuracy achieved by Naive Bayes Classifier - 86.5%\n",
    "    Accuracy achieved by Random Forest Classifier - 78.7%\n",
    "\n",
    "We can see that both the algorithms easily beat the baseline accuracy, but the Naive Bayes Classifier outperforms the Random Forest Classifier with approximately 87% accuracy. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63f9911de6f9547d26d6ccd5b997d4c24c9ba8fc8843a26db797dceee8b60692"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
